{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73b821a3",
   "metadata": {},
   "source": [
    "#### Both in stemming and in lemmatization, try to reduce a given word to its root word. \n",
    "\n",
    "#### The root word is called a stem in the stemming process, and it is called a lemma in the lemmatization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0948c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6746dafb",
   "metadata": {},
   "source": [
    "### Lemmatization is derived from lemma, and the lemma of a word corresponds to its dictionary form. \n",
    "### Lemma of words are created depending on their meaning (adjective, a noun, or a verb.) in the text they are being used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ce490cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walked\n",
      "university\n",
      "fly\n",
      "walk\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "words = ['Walked', 'university', 'flying', 'walked']\n",
    "\n",
    "for w in words:\n",
    "    print(lemma.lemmatize(w, pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527936b9",
   "metadata": {},
   "source": [
    "#### Why is the first element the same in the output?\n",
    "#### because, first we have to make it lowercase then tokenize. \n",
    "#### now let's see another example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "25f64abb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['history', 'be', 'best', 'for', 'learn', 'about', 'life', 'and', 'people', '.', 'we', 'be', 'all', 'part', 'of', 'history']\n"
     ]
    }
   ],
   "source": [
    "# A Perfect Example\n",
    "\n",
    "words = 'History is best for learning about life and people. We are all part of history'\n",
    "\n",
    "lower = words.lower()\n",
    "token = word_tokenize(lower)\n",
    "rejoin_words = []\n",
    "\n",
    "for t in token:\n",
    "    rejoin_words.append(lemma.lemmatize(t, pos='v'))\n",
    "print(rejoin_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "d19af2cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History —-> History\n",
      "is —-> be\n",
      "the —-> the\n",
      "best —-> best\n",
      "subject —-> subject\n",
      "for —-> for\n",
      "teaching —-> teach\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "WordNetLemmatizer = WordNetLemmatizer()\n",
    "\n",
    "sent = 'History is the best subject for teaching'\n",
    "tokens = nltk.word_tokenize(sent)\n",
    "\n",
    "for t in tokens:\n",
    "    print(t,'—->', WordNetLemmatizer.lemmatize(t, pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915eb762",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7c3cf2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: Absorb --->: Absorb\n",
      "word: what --->: what\n",
      "word: is --->: is\n",
      "word: useful, --->: useful,\n",
      "word: discard --->: discard\n",
      "word: what --->: what\n",
      "word: is --->: is\n",
      "word: not, --->: not,\n",
      "word: add --->: add\n",
      "word: what --->: what\n",
      "word: is --->: is\n",
      "word: uniquely --->: uniquely\n",
      "word: your --->: your\n",
      "word: own. --->: own.\n"
     ]
    }
   ],
   "source": [
    "# using function\n",
    "\n",
    "philosophy = 'Absorb what is useful, discard what is not, add what is uniquely your own.'\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "def lemma_words(sentence, model=lemma):\n",
    "    for word in sentence.split():\n",
    "        lemma = model.lemmatize(word)\n",
    "        print('word: {} --->: {}'.format(word, lemma))\n",
    "    \n",
    "lemma_words(philosophy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f97bfa6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6700da5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['No', 'step', 'be', 'small', ',', 'as', 'long', 'as', 'it', 'be', 'head', 'in', 'the', 'direction', 'of', 'where', 'we', 'want', 'to', 'go']\n"
     ]
    }
   ],
   "source": [
    "# We can see that no is printed as uppercase.\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = 'No step is small, as long as it is headed in the direction of where we want to go'\n",
    "token = word_tokenize(text)\n",
    "quotes = []\n",
    "\n",
    "for t in token:\n",
    "    quotes.append(lemma.lemmatize(t, pos='v'))\n",
    "print(quotes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab7f08e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ff376c43",
   "metadata": {},
   "source": [
    "### Stemming Vs Lemmetization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "501f50c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Word: fly\n",
      "Stemmed Word: fli\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lem = WordNetLemmatizer()\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "stem = PorterStemmer()\n",
    "\n",
    "word = \"flying\"\n",
    "print(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\n",
    "print(\"Stemmed Word:\",stem.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "90b74122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Stemming: ['a', 'smart', 'kid', 'ran', 'toward', 'the', 'polic', 'station', 'when', 'he', 'saw', 'the', 'thiev', 'approach', '.'] \n",
      "\n",
      "After Lemmatization: ['A', 'smart', 'kid', 'ran', 'towards', 'the', 'police', 'station', 'when', 'he', 'saw', 'the', 'thief', 'approaching', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stem = PorterStemmer()\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "text = \"A smart kid ran towards the police station when he saw the thieves approaching.\"\n",
    "token = word_tokenize(text)\n",
    "\n",
    "stemming = [stem.stem(w) for w in token]\n",
    "print('After Stemming:', stemming, '\\n')\n",
    "\n",
    "lemmatization = [lemma.lemmatize(w) for w in token]\n",
    "print('After Lemmatization:', lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dbca37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "52ebc8c0",
   "metadata": {},
   "source": [
    "### Apart from the time consuming issue, Lemmetization is much better than stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b418c96a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ba6479",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5aadf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c194c22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9900d65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99302508",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7563045e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50841529",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad34948",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6796ee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d08668",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b03ee9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
